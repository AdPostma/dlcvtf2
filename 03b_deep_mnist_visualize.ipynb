{
 "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I-S9EwVjCGwX",
        "colab_type": "text"
      },
      "source": [
        "# Deep Learning for Computer Vision with TensorFlow 2.0\n",
        "\n",
        "Table of contents:\n",
        "[Lab 0](https://colab.research.google.com/github/embedded-vision/dlcvtf2/blob/master/00_test_install.ipynb) | \n",
        "[Lab 1](https://colab.research.google.com/github/embedded-vision/dlcvtf2/blob/master/01_linear_regression.ipynb) | \n",
        "[Lab 2](https://colab.research.google.com/github/embedded-vision/dlcvtf2/blob/master/02_tensorflow_logistic_regression.ipynb) | \n",
        "[Lab 3a](https://colab.research.google.com/github/embedded-vision/dlcvtf2/blob/master/03a_tensorflow_deep_network.ipynb) | \n",
        "[Lab 3b](https://colab.research.google.com/github/embedded-vision/dlcvtf2/blob/master/03b_deep_mnist_visualize.ipynb) | \n",
        "[Lab 4](https://colab.research.google.com/github/embedded-vision/dlcvtf2/blob/master/04_mnist_cnn.ipynb) | \n",
        "[Lab 5](https://colab.research.google.com/github/embedded-vision/dlcvtf2/blob/master/05_data_prep.ipynb) | \n",
        "[Lab 6](https://colab.research.google.com/github/embedded-vision/dlcvtf2/blob/master/06_transfer_learning.ipynb) | \n",
        "\n",
        "# Lab 3B: Deep MNIST with Visualization\n",
        "\n",
        "If you ever want to start over just go to the menu at the top and select Runtime ->  Restart runtime.  (Please do not use Runtime -> Reset all runtimes.)"
      ]
    },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "In this exercise you will use the deep model used in the last exercise but be we are going to closely monitor the training process.  \n",
    "\n",
    "The training process is setup similar to the previous example, but there is an issue with one of the cells. It will be pointed out, but run it as provided and update the model as instructed later to make it work properly. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 1\n",
    "# notebook version 1.3\n",
    "!pip install tensorflow==2.0.0-alpha0 \n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "import numpy as np\n",
    "%pylab inline\n",
    "\n",
    "print ('cell finished')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell sets up the default settings for our training variables once again.  \n",
    "\n",
    "The default starting value of the `LEARNING_RATE` variable is wrong on purpose. This is to let us see what happens in the training process when something goes wrong. Run through the process once to see the abysmal values, then come back to this cell and change `LEARNING_RATE` to .001. \n",
    "\n",
    "Run the process again to see that it is working properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 2\n",
    "\n",
    "BATCH_SIZE=100\n",
    "\n",
    "HIDDEN_SIZE = 300\n",
    "LEARNING_RATE = 0.5 # This is much too large, but run with it initally and change it later as instructed\n",
    "\n",
    "NUM_CLASSES = 10\n",
    "IMG_HT = 28\n",
    "IMG_W = 28\n",
    "\n",
    "print ('cell finished')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell reads in the MNIST dataset as before and sets up our model parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 3\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "# Numpy defaults to dtype=float64; TF defaults to float32. Stick with float32.\n",
    "x_train, x_test = x_train / np.float32(255), x_test / np.float32(255)\n",
    "y_train, y_test = y_train.astype(np.int64), y_test.astype(np.int64)\n",
    "\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train)).repeat()\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
    "\n",
    "# DON'T CHANGE THESE VARIABLES OR YOU WILL BREAK THINGS\n",
    "NUM_PIXELS = 28 * 28\n",
    "NUM_CLASSES = 10\n",
    "\n",
    "print ('cell finished')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the same code to setup our placeholders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 4\n",
    "\n",
    "train_ds = train_ds.shuffle(60000).batch(BATCH_SIZE)\n",
    "test_ds = test_ds.batch(BATCH_SIZE)\n",
    "\n",
    "print ('cell finished')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell is the same model as before. It uses the constants from cell 2 to size the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 5\n",
    "\n",
    "def model():\n",
    "    model = keras.Sequential([\n",
    "    layers.Flatten(input_shape=[IMG_HT,IMG_W]),\n",
    "    layers.Dense(HIDDEN_SIZE, activation='relu'),\n",
    "    layers.Dense(NUM_CLASSES, activation='softmax')\n",
    "  ])\n",
    "    return model\n",
    "\n",
    "tb_callback = tf.keras.callbacks.TensorBoard(log_dir='log_dir')\n",
    "\n",
    "print ('cell finished')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the model and displaying the summary as before "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 6\n",
    "\n",
    "mnist_model = model()\n",
    "\n",
    "mnist_model.summary()\n",
    "\n",
    "print ('cell finished')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell creates an optimizer directly using the `LEARNING_RATE` constant from cell 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(LEARNING_RATE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is compiled and instead of specifying the built-in optimizer with default values the optimizer object created above is used as the optimizer argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 7\n",
    "\n",
    "mnist_model.compile(optimizer=optimizer, \n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print ('cell finished')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the model will be fit to the data using our standard technique. However the first time through the loss will be high and the accuracy low. After the learning process is fixed, the second time through should show much better results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 8\n",
    "\n",
    "history = mnist_model.fit(\n",
    "  train_ds,\n",
    "  epochs=20,  \n",
    "  steps_per_epoch=600,verbose=2,callbacks=[tb_callback])\n",
    "\n",
    "print ('cell finished')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the fit method is run data about the run is returned. In our example this data is gathered in an object named `history` which was provided as the output variable from the fit process. \n",
    "\n",
    "The data is a dictionary object whose contents are based on the metrics requested for the training process. In this example the accuracy `acc` and loss `loss` were captured and can be plot over each epoch of the training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 9\n",
    "#pylab.plot(sqft_train, price_train, 'b.')\n",
    "\n",
    "pylab.plot(history.history['accuracy'],'b')\n",
    "pylab.title('Model accuracy')\n",
    "pylab.ylabel('Accuracy')\n",
    "pylab.xlabel('Epoch')\n",
    "pylab.legend(['Train'], loc='upper left')\n",
    "pylab.show()\n",
    "\n",
    "# Plot training & validation loss values\n",
    "pylab.plot(history.history['loss'], 'r')\n",
    "#plt.plot(history.history['val_loss'])\n",
    "pylab.title('Model loss')\n",
    "pylab.ylabel('Loss')\n",
    "pylab.xlabel('Epoch')\n",
    "pylab.legend(['Train'], loc='upper left')\n",
    "pylab.show()\n",
    "\n",
    "print ('cell finished')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plots above were for the accuracy and loss over time for each epoch. Below the accuracy of model using the testing data will be evaluated using the evaluate method. A history object is once again returned but since the evaluation only runs for one epoch, the history object will return one value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 10\n",
    "\n",
    "history=mnist_model.evaluate(\n",
    "    test_ds, steps=100)\n",
    "\n",
    "\n",
    "print ('\\n cell finished')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell just reformats the data for ease of understanding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 11\n",
    "\n",
    "print ('loss = {} accuracy = {}'.format (history[0], history[1]))\n",
    "\n",
    "print ('cell finished')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see the accuracy is terrible. Something is obviously wrong with the learning process. While there are lots of things that can go wrong, such as bad data, bad data pipelining, etc. hyperparameter settings can also affect the output. In this case the `LEARNING_RATE` is much too high for the first pass. \n",
    "\n",
    "Do the following:\n",
    "\n",
    "1. Restart the notebook - from the Kernel menu at the top of the page select **Restart Runtime**\n",
    "2. Change LEARNING_RATE - go to cell 2 and modify LEARNING_RATE to 0.001\n",
    "3. Re-run all cells - start at cell 1 and run every cell in the notebook to cell 11 **Note: don't use the option Restart and Run All as cells depend on one another and this usually won't work properly**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## If you have time\n",
    "You can experiment with other learning rates to see what happens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
